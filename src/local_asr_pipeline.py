"""
CounselNote æœ¬åœ° ASR èˆ‡æ‘˜è¦è‡ªå‹•åŒ–å·¥å…·ã€‚

åŠŸèƒ½ï¼š
- å°‡å–®ä¸€éŸ³æª”æˆ–æ•´å€‹è³‡æ–™å¤¾æ‰¹æ¬¡é€²è¡Œè½‰éŒ„ï¼ˆASRï¼‰èˆ‡æ‘˜è¦åŒ¯å‡º
- åˆ©ç”¨ç¾æœ‰çš„ TXT/JSON å¿«å–ï¼Œå¿…è¦æ™‚å¯åŠ ä¸Š --force å¼·åˆ¶é‡è·‘
- æ”¯æ´ LM Studioã€Ollama å…©ç¨®æœ¬åœ° LLMï¼Œçµæœå¯«å…¥æŒ‡å®šè¼¸å‡ºè³‡æ–™å¤¾

åŸºæœ¬ç”¨æ³•ï¼š
    python src/local_asr_pipeline.py <path> [é¸é …]

å¸¸ç”¨é¸é …ï¼š
    --provider {lmstudio, ollama}    é¸æ“‡æ‘˜è¦æ‰€ä½¿ç”¨çš„ LLM æœå‹™
    --lmstudio_model NAME            æŒ‡å®š LM Studio æ¨¡å‹ï¼ˆé è¨­ qwen2.5-7b-instructï¼‰
    --ollama_model NAME              æŒ‡å®š Ollama æ¨¡å‹ï¼ˆé è¨­ qwen3:4bï¼‰
    --out_dir PATH                   è¼¸å‡ºè³‡æ–™å¤¾ï¼Œé è¨­ outputs
    --ext LIST                       æ‰¹æ¬¡æ¨¡å¼æ”¯æ´çš„å‰¯æª”åæ¸…å–®ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰
    --force                          å³ä½¿å·²æœ‰ TXT/JSON ä¹Ÿé‡æ–°è½‰éŒ„èˆ‡æ‘˜è¦

ç¯„ä¾‹ï¼š
    python src/local_asr_pipeline.py D:/audio --provider ollama
    python src/local_asr_pipeline.py D:/audio/case.mp3 --provider lmstudio --force
"""

import os
import re
import json
import argparse
import time
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional
import requests
from faster_whisper import WhisperModel

# ======== é è¨­åƒæ•¸ ========
ASR_MODEL_SIZE = "large-v3"
DEVICE = "cuda"
COMPUTE_TYPE = "int8"     # 1080Ti å»ºè­° int8 æˆ– int8_float16
LMSTUDIO_API = "http://localhost:1234/v1/chat/completions"
LMSTUDIO_MODEL = "qwen2.5-7b-instruct"
OLLAMA_API = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "qwen3:4b"

INIT_PROMPT = (
    "ä»¥ä¸‹æ˜¯å¤§å­¸è€å¸«èˆ‡å¤§ä¸€å­¸ç”Ÿçš„ä¸€å°ä¸€è¼”å°è«‡è©±é€å­—ç¨¿ã€‚è«‹ç”¨æ­£é«”ä¸­æ–‡ç†è§£å£èªï¼Œä¿ç•™æ•™è‚²å ´åŸŸèªå¢ƒã€‚"
)
SUMMARY_PROMPT = """
ä½ æ˜¯ä¸€ä½å­¸è¼”ç´€éŒ„åŠ©ç†ã€‚è«‹æ ¹æ“šã€Œé€å­—ç¨¿ã€ç”¢å‡ºçµæ§‹åŒ–çµæœã€‚

è«‹åªè¼¸å‡ºä¸€å€‹ JSON ç‰©ä»¶ï¼ˆä¸å¾—æœ‰ä»»ä½•å‰å¾Œè§£èªªæ–‡å­—ã€æ¨™é»æˆ–ç¨‹å¼ç¢¼åœæ¬„ï¼‰ï¼Œéµèˆ‡å‹åˆ¥ **å¿…é ˆ** å®Œå…¨ç¬¦åˆä»¥ä¸‹è¦ç¯„ï¼š
{
  "summary": "<ä»¥ç¹é«”ä¸­æ–‡æ’°å¯«ï¼Œ350â€“450å­—ã€‚è«‹åœ¨æ‘˜è¦ä¸­æ¸…æ¥šå€åˆ†è€å¸«èˆ‡å­¸ç”Ÿçš„è§€é»ï¼é‡é»>",
  "categories": [ "<å¾ä¸‹åˆ—å€™é¸å€¼ä¸­æ“‡ä¸€æˆ–å¤šå€‹ï¼šèª²æ¥­ã€ç”Ÿæ´»ã€äº¤å‹ã€å¿ƒç†ã€ç”Ÿæ¶¯ã€èª²å¤–æ´»å‹•>", ... ],
  "risk_flags": [ "<å¯ç•™ç©ºé™£åˆ—ï¼›å¦‚æœ‰ï¼Œåˆ—å‡ºå…·é«”é¢¨éšªé»ï¼Œå¦‚ï¼šé•·æœŸå¤±çœ ã€ç–‘ä¼¼ç„¦æ…®ã€è‡ªå‚·å¿µé ­ç­‰>", ... ],
  "followups": [ "<æ•™å¸«å¾ŒçºŒè¿½è¹¤è¡Œå‹•å»ºè­°ï¼ˆæ¢åˆ—åˆ—é»ï¼‰>", ... ]
}

è¦å‰‡ï¼š
- ä¸å¾—è¼¸å‡ºä»»ä½•è§£èªªã€æ­¥é©Ÿã€åˆ†æã€æ¨ç†æˆ–æ€è€ƒå…§å®¹ï¼›**åªå…è¨±è¼¸å‡ºå–®ä¸€ JSON ç‰©ä»¶**ã€‚
- åªèƒ½è¼¸å‡º **ä¸€å€‹** JSON ç‰©ä»¶ï¼Œä¸”å¿…é ˆæ˜¯åˆæ³• JSONï¼ˆé›™å¼•è™Ÿã€é€—è™Ÿèˆ‡æ‹¬è™Ÿä½ç½®æ­£ç¢ºï¼‰ã€‚
- "categories" é™£åˆ—çš„å…ƒç´ å¿…é ˆåªä¾†è‡ªä»¥ä¸‹å€™é¸å€¼ï¼š["èª²æ¥­","ç”Ÿæ´»","äº¤å‹","å¿ƒç†","ç”Ÿæ¶¯","èª²å¤–æ´»å‹•"]ï¼›å¯å¤šé¸æˆ–çµ¦ç©ºé™£åˆ—ã€‚
- "summary" å¿…é ˆ 350â€“450 å€‹ä¸­æ–‡å­—ï¼Œé¿å…å€‹è³‡ï¼›å¿…è¦æ™‚å¯ç”¨ã€Œå­¸ç”Ÿã€ã€Œè€å¸«ã€æŒ‡ç¨±ã€‚
- ä¸å¾—æ–°å¢é™¤äº† "summary","categories","risk_flags","followups" ä»¥å¤–çš„éµã€‚
"""
# ===========================

_WHISPER_MODEL = None
_WHISPER_MODEL_DEVICE = None
_WHISPER_MODEL_COMPUTE = None

def get_whisper_model() -> Tuple[WhisperModel, str, str]:
    """è¼‰å…¥ Whisper æ¨¡å‹ï¼Œè‹¥ CUDA å¤±æ•—å‰‡æ”¹ç”¨ CPUã€‚"""
    global _WHISPER_MODEL, _WHISPER_MODEL_DEVICE, _WHISPER_MODEL_COMPUTE
    if _WHISPER_MODEL is not None:
        return _WHISPER_MODEL, _WHISPER_MODEL_DEVICE, _WHISPER_MODEL_COMPUTE

    device = DEVICE
    compute = COMPUTE_TYPE
    try:
        model = WhisperModel(ASR_MODEL_SIZE, device=device, compute_type=compute)
    except Exception as exc:
        if device != "cuda":
            raise
        fallback_device = "cpu"
        fallback_compute = "int8"
        print(f"[warn] CUDA åˆå§‹åŒ–å¤±æ•—ï¼ˆ{exc}ï¼‰ï¼Œæ”¹ç”¨ CPU int8 é‡æ–°è¼‰å…¥æ¨¡å‹ã€‚")
        model = WhisperModel(ASR_MODEL_SIZE, device=fallback_device, compute_type=fallback_compute)
        device = fallback_device
        compute = fallback_compute

    _WHISPER_MODEL = model
    _WHISPER_MODEL_DEVICE = device
    _WHISPER_MODEL_COMPUTE = compute
    return model, device, compute

def transcribe(audio_path: str) -> Tuple[str, float]:
    """faster-whisper èªéŸ³è½‰æ–‡å­—"""
    model, device_used, compute_used = get_whisper_model()
    print(f"[info] start transcription device={device_used} compute={compute_used}: {audio_path}")
    segments, info = model.transcribe(
        audio_path,
        language="zh",
        initial_prompt=INIT_PROMPT,
        vad_filter=True,
        beam_size=5,
    )
    lines = []
    for seg in segments:
        mm = int(seg.start // 60)
        ss = int(seg.start % 60)
        ts = f"[{mm:02d}:{ss:02d}] "
        lines.append(ts + (seg.text or "").strip())
    transcript = "\n".join(lines)
    print(f"[info] transcription finished {audio_path} ({info.duration:.1f}s)")
    return transcript, info.duration

def call_lmstudio(model: str, content: str) -> str:
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯åš´è¬¹çš„å­¸è¼”æ‘˜è¦åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": SUMMARY_PROMPT + "\n\né€å­—ç¨¿ï¼š\n" + content}
        ],
        "temperature": 0.2,
        "max_tokens": 512,
        "stream": False
    }
    r = requests.post(LMSTUDIO_API, json=payload, timeout=600)
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"]

def call_ollama(model: str, content: str) -> str:
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯åš´è¬¹çš„å­¸è¼”æ‘˜è¦åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": SUMMARY_PROMPT + "\n\né€å­—ç¨¿ï¼š\n" + content}
        ],
        "stream": False,
        "options": {"temperature": 0.2}
    }
    r = requests.post(OLLAMA_API, json=payload, timeout=600)
    r.raise_for_status()
    return r.json()["message"]["content"]

def extract_json_block(text: str) -> str:
    text = text.strip()
    if text.startswith("{") and text.endswith("}"):
        return text
    start_idx, end_idx = text.find("{"), text.rfind("}")
    if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
        return text[start_idx:end_idx+1]
    return text

def coerce_list(x) -> List[str]:
    if x is None:
        return []
    if isinstance(x, list):
        return [str(i) for i in x]
    if isinstance(x, str):
        parts = [p.strip() for p in re.split(r"[ï¼Œ,]\s*", x) if p.strip()]
        return parts if parts else ([x] if x else [])
    return [str(x)]

def text_after_last_think(text: str) -> str:
    """
    å›å‚³æœ€å¾Œä¸€å€‹ </think> æ¨™ç±¤ä¹‹å¾Œçš„æ–‡å­—ã€‚
    è‹¥ä¸å­˜åœ¨ </think>ï¼ˆå¤§å°å¯«ä¸æ‹˜ï¼‰ï¼Œå‰‡å›å‚³åŸæ–‡ã€‚
    """
    matches = list(re.finditer(r"</\s*think\s*>", text, flags=re.IGNORECASE))
    if not matches:
        return text
    last_end = matches[-1].end()
    return text[last_end:].lstrip()
    
def summarize(provider: str, model: str, transcript: str) -> Dict[str, Any]:
    raw = call_lmstudio(model, transcript) if provider == "lmstudio" else call_ollama(model, transcript)
    # å…ˆæ“·å–æœ€å¾Œä¸€å€‹ </think> ä¹‹å¾Œçš„å…§å®¹ï¼ˆé¿é–‹å¯èƒ½çš„æ€è€ƒéç¨‹é›œè¨Šï¼Œä¾‹å¦‚qwen3:4bï¼‰
    raw = text_after_last_think(raw)

    json_str = extract_json_block(raw)
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError as e:
        raise ValueError(f"LLM å›å‚³é JSONï¼š{e}\nåŸå§‹ï¼š\n{raw}")
    return {
        "summary": str(data.get("summary", "")).strip(),
        "categories": coerce_list(data.get("categories", [])),
        "risk_flags": coerce_list(data.get("risk_flags", [])),
        "followups": coerce_list(data.get("followups", []))
    }

def preferred_transcript_path(audio_path: str, out_dir: str) -> str:
    base = os.path.splitext(os.path.basename(audio_path))[0]
    return os.path.join(out_dir, base + ".txt")

def find_existing_transcript(audio_path: str, out_dir: str) -> Optional[str]:
    cand1 = preferred_transcript_path(audio_path, out_dir)
    if os.path.isfile(cand1):
        return cand1
    base = os.path.splitext(os.path.basename(audio_path))[0]
    cand2 = os.path.join(os.path.dirname(audio_path), base + ".txt")
    return cand2 if os.path.isfile(cand2) else None







def process_one(audio_path: str, provider: str, lmstudio_model: str, ollama_model: str, out_dir: str, force: bool) -> Tuple[bool, str]:
    trans_duration = 0.0
    summary_duration = 0.0
    trans_start: Optional[float] = None
    summary_start: Optional[float] = None
    summary_started = False
    try:
        os.makedirs(out_dir, exist_ok=True)
        base = os.path.splitext(os.path.basename(audio_path))[0]
        existing_txt = find_existing_transcript(audio_path, out_dir)

        trans_start = time.time()
        if existing_txt and not force:
            with open(existing_txt, "r", encoding="utf-8") as f:
                transcript_txt = f.read()
            print(f"â­ï¸ å·²å­˜åœ¨é€å­—ç¨¿ï¼š{existing_txt}ï¼ˆ--force æœªæŒ‡å®šï¼Œè·³é ASRï¼‰")
            seconds = 0
            trans_duration = 0.0
        else:
            transcript_txt, seconds = transcribe(audio_path)
            out_txt = preferred_transcript_path(audio_path, out_dir)
            with open(out_txt, "w", encoding="utf-8") as f:
                f.write(transcript_txt)
            trans_duration = time.time() - trans_start
            print(f"ğŸ§ åŸ·è¡Œ ASRï¼š{audio_path}")

        out_json = os.path.join(out_dir, base + ".json")
        if os.path.isfile(out_json) and not force:
            print(f"â­ï¸ å·²å­˜åœ¨æ‘˜è¦ï¼š{out_json}ï¼ˆ--force æœªæŒ‡å®šï¼Œè·³éæ‘˜è¦ï¼‰")
            summary_duration = 0.0
        else:
            summary_start = time.time()
            summary_started = True
            s = summarize(provider, lmstudio_model if provider == "lmstudio" else ollama_model, transcript_txt)
            summary_duration = time.time() - summary_start

            final_obj = {
                "file": audio_path,
                "processed_at": datetime.fromtimestamp(os.path.getmtime(audio_path)).isoformat(timespec="seconds"),
                "duration_sec": int(seconds),
                "transcript_txt": transcript_txt,
                "summary": s["summary"],
                "categories": s["categories"],
                "risk_flags": s["risk_flags"],
                "followups": s["followups"]
            }
            with open(out_json, "w", encoding="utf-8") as f:
                json.dump(final_obj, f, ensure_ascii=False, indent=2)

        print(f"[time] transcript {trans_duration:.1f}s | summary {summary_duration:.1f}s")
        print(f"âœ… å®Œæˆï¼š{audio_path} â†’ {out_json}")
        return True, out_json
    except Exception as e:
        now = time.time()
        if trans_start is not None and trans_duration == 0.0:
            trans_duration = now - trans_start
        if summary_started and summary_start is not None and summary_duration == 0.0:
            summary_duration = now - summary_start
        print(f"[time] transcript {trans_duration:.1f}s | summary {summary_duration:.1f}s (å¤±æ•—)")
        return False, f"{audio_path} å¤±æ•—ï¼š{e}"

def find_audio_files(path: str, exts: List[str]) -> List[str]:
    if os.path.isfile(path):
        return [path]
    files: List[str] = []
    for root, _, names in os.walk(path):
        for name in names:
            if name.lower().endswith(tuple(exts)):
                files.append(os.path.join(root, name))
    return sorted(files)

def main():
    parser = argparse.ArgumentParser(description="ASRâ†’LLM æ‘˜è¦ï¼ˆæ”¯æ´æ‰¹æ¬¡ï¼Œ--force å¯å¼·åˆ¶é‡è·‘ ASRï¼‰")
    parser.add_argument("path")
    parser.add_argument("--provider", choices=["lmstudio", "ollama"], default="ollama")
    parser.add_argument("--lmstudio_model", default=LMSTUDIO_MODEL)
    parser.add_argument("--ollama_model", default=OLLAMA_MODEL)
    parser.add_argument("--out_dir", default="outputs")
    parser.add_argument("--ext", default="mp3,wav,m4a,aac,flac")
    parser.add_argument("--force", action="store_true", help="å³ä½¿å·²æœ‰é€å­—ç¨¿ä¹Ÿå¼·åˆ¶é‡è·‘ ASR")
    args = parser.parse_args()

    exts = [("." + ext.strip().lstrip(".")).lower() for ext in args.ext.split(",") if ext.strip()]
    targets = find_audio_files(args.path, exts)
    if not targets:
        raise FileNotFoundError("æ‰¾ä¸åˆ°éŸ³æª”")

    print(f"ğŸ” æ‰¾åˆ° {len(targets)} å€‹éŸ³æª”ï¼Œforce={args.force}")
    overall_start = time.time()
    ok, fail = 0, 0
    errors: List[str] = []

    for idx, fp in enumerate(targets, 1):
        print(f"\n[{idx}/{len(targets)}] è™•ç†ï¼š{fp}")
        success, info = process_one(fp, args.provider, args.lmstudio_model, args.ollama_model, args.out_dir, args.force)
        if success:
            ok += 1
        else:
            fail += 1
            errors.append(info)
            print("âŒ", info)

    total_elapsed = time.time() - overall_start
    print(f"\n===== æ‰¹æ¬¡ç¸½çµ =====\næˆåŠŸï¼š{ok}, å¤±æ•—ï¼š{fail}")
    print(f"[time] å…¨éƒ¨è™•ç†è€—æ™‚ {total_elapsed:.1f}s")

    if errors:
        print("å¤±æ•—æ¸…å–®ï¼š")
        for err in errors:
            print(" -", err)

if __name__ == "__main__":
    main()
